{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jasme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jasme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jasme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "from nltk.tag import pos_tag # for proper noun\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import math\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "ps = PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ......................part 1 (cue phrases).................\n",
    "def cue_phrases():\n",
    "    QPhrases=[\"incidentally\", \"example\", \"anyway\", \"furthermore\",\"according\"\n",
    "            \"first\", \"second\", \"then\", \"now\", \"thus\", \"moreover\", \"therefore\", \"hence\", \"lastly\", \"finally\", \"summary\"]\n",
    "\n",
    "    cue_phrases={}\n",
    "    for sentence in sent_tokens:\n",
    "        cue_phrases[sentence] = 0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        for word in word_tokens:\n",
    "            if word.lower() in QPhrases:\n",
    "                cue_phrases[sentence] += 1\n",
    "    maximum_frequency = max(cue_phrases.values())\n",
    "    for k in cue_phrases.keys():\n",
    "        try:\n",
    "            cue_phrases[k] = cue_phrases[k] / maximum_frequency\n",
    "            cue_phrases[k]=round(cue_phrases[k],3)\n",
    "        except ZeroDivisionError:\n",
    "            x=0\n",
    "    print(cue_phrases.values())\n",
    "    return cue_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .......................part2 (numerical data)...................\n",
    "def numeric_data():\n",
    "    numeric_data={}\n",
    "    for sentence in sent_tokens:\n",
    "        numeric_data[sentence] = 0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        for k in word_tokens:\n",
    "            if k.isdigit():\n",
    "                numeric_data[sentence] += 1\n",
    "    maximum_frequency = max(numeric_data.values())\n",
    "    for k in numeric_data.keys():\n",
    "        try:\n",
    "            numeric_data[k] = (numeric_data[k]/maximum_frequency)\n",
    "            numeric_data[k] = round(numeric_data[k], 3)\n",
    "        except ZeroDivisionError:\n",
    "            x=0\n",
    "    print(numeric_data.values())\n",
    "    return numeric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#....................part3(sentence length)........................\n",
    "def sent_len_score():\n",
    "    sent_len_score={}\n",
    "    for sentence in sent_tokens:\n",
    "        sent_len_score[sentence] = 0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        if len(word_tokens) in range(0,10):\n",
    "            sent_len_score[sentence]=1-0.05*(10-len(word_tokens))\n",
    "        elif len(word_tokens) in range(7,20):\n",
    "            sent_len_score[sentence]=1\n",
    "        else:\n",
    "            sent_len_score[sentence]=1-(0.05)*(len(word_tokens)-20)\n",
    "    for k in sent_len_score.keys():\n",
    "        sent_len_score[k]=round(sent_len_score[k],4)\n",
    "    print(sent_len_score.values())\n",
    "    return sent_len_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#....................part4(sentence position)........................\n",
    "def sentence_position():\n",
    "    sentence_position={}\n",
    "    d=1\n",
    "    no_of_sent=len(sent_tokens)\n",
    "    for i in range(no_of_sent):\n",
    "        a=1/d\n",
    "        b=1/(no_of_sent-d+1)\n",
    "        sentence_position[sent_tokens[d-1]]=max(a,b)\n",
    "        d=d+1\n",
    "    for k in sentence_position.keys():\n",
    "        sentence_position[k]=round(sentence_position[k],3)\n",
    "    print(sentence_position.values())\n",
    "    return sentence_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a frequency table to compute the frequency of each word.\n",
    "def word_frequency():\n",
    "    freqTable = {}\n",
    "    for word in word_tokens_refined:    \n",
    "        if word in freqTable:         \n",
    "            freqTable[word] += 1    \n",
    "        else:         \n",
    "            freqTable[word] = 1\n",
    "    for k in freqTable.keys():\n",
    "        freqTable[k]= math.log10(1+freqTable[k])\n",
    "#Compute word frequnecy score of each sentence\n",
    "    word_frequency={}\n",
    "    for sentence in sent_tokens:\n",
    "        word_frequency[sentence]=0\n",
    "        e=nltk.word_tokenize(sentence)\n",
    "        f=[]\n",
    "        for word in e:\n",
    "            f.append(ps.stem(word))\n",
    "        for word,freq in freqTable.items():\n",
    "            if word in f:\n",
    "                word_frequency[sentence]+=freq\n",
    "    maximum=max(word_frequency.values())\n",
    "    for key in word_frequency.keys():\n",
    "        try:\n",
    "            word_frequency[key]=word_frequency[key]/maximum\n",
    "            word_frequency[key]=round(word_frequency[key],3)\n",
    "        except ZeroDivisionError:\n",
    "            x=0\n",
    "    print(word_frequency.values())\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#........................part 6 (upper cases).................................\n",
    "def upper_case():\n",
    "    upper_case={}\n",
    "    for sentence in sent_tokens:\n",
    "        upper_case[sentence] = 0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        for k in word_tokens:\n",
    "            if k.isupper():\n",
    "                upper_case[sentence] += 1\n",
    "    maximum_frequency = max(upper_case.values())\n",
    "    for k in upper_case.keys():\n",
    "        try:\n",
    "            upper_case[k] = (upper_case[k]/maximum_frequency)\n",
    "            upper_case[k] = round(upper_case[k], 3)\n",
    "        except ZeroDivisionError:\n",
    "            x=0\n",
    "    print(upper_case.values())\n",
    "    return upper_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#......................... part7 (number of proper noun)...................\n",
    "def proper_noun():\n",
    "    proper_noun={}\n",
    "    for sentence in sent_tokens:\n",
    "        tagged_sent = pos_tag(sentence.split())\n",
    "        propernouns = [word for word, pos in tagged_sent if pos == 'NNP']\n",
    "        proper_noun[sentence]=len(propernouns)\n",
    "    maximum_frequency = max(proper_noun.values())\n",
    "    for k in proper_noun.keys():\n",
    "        try:\n",
    "            proper_noun[k] = (proper_noun[k]/maximum_frequency)\n",
    "            proper_noun[k] = round(proper_noun[k], 3)\n",
    "        except ZeroDivisionError:\n",
    "            x=0\n",
    "    print(proper_noun.values())\n",
    "    return proper_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#................................. part 8 (word matches with heading) ...................\n",
    "def head_match():\n",
    "    head_match={}\n",
    "    heading=sent_tokens[0]\n",
    "    for sentence in sent_tokens:\n",
    "        head_match[sentence]=0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        for k in word_tokens:\n",
    "            if k not in stopWords:\n",
    "                k = ps.stem(k)\n",
    "                if k in ps.stem(heading):\n",
    "                    head_match[sentence] += 1\n",
    "    maximum_frequency = max(head_match.values())\n",
    "    for k in head_match.keys():\n",
    "        try:\n",
    "            head_match[k] = (head_match[k]/maximum_frequency)\n",
    "            head_match[k] = round(head_match[k], 3)\n",
    "        except ZeroDivisionError:\n",
    "            x=0\n",
    "    print(head_match.values())\n",
    "    return head_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [a, b, c, d, upper, f, g, h, key, label]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(columns=['a','b','c','d','upper','f','g','h','key','label'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0])\n",
      "dict_values([0.8, -2.25, 1, -0.1, 1.0, 1, 0.6, 0.35, 1, 0.85])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0])\n",
      "dict_values([0.87, 1.0, 0.304, 0.478, 0.13, 0.174, 0.13, 0.13, 0.261, 0.217])\n",
      "dict_values([6.902185713699001, 14.252334453376452, 2.6946051989335693, 10.764913242016975, 3.391640703492388, 2.24551266781415, 4.345883212931713, 2.7226339225338125, 3.023663918197794, 3.743823221603751])\n",
      "dict_values([0.364, 1.0, 0.0, 0.909, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0])\n",
      "dict_values([1, 0.6, 1, 0.8, 0.8, -0.05, 1, 1, 1.0, 0.45, -0.35, -0.8, 1, 0.4, 0.15, 0.5])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.333, 0.667, 0.333, 0.667, 0.0, 0.0, 0.667, 0.0, 0.333, 0.333, 0.0, 0.333, 0.0, 0.0, 1.0, 0.333])\n",
      "dict_values([1.0, 0.455, 0.182, 0.182, 0.182, 0.273, 0.182, 0.182, 0.273, 0.091, 0.091, 0.364, 0.091, 0.182, 0.364, 0.364])\n",
      "dict_values([4.548512256341035, 8.280906016164003, 4.388811413473523, 4.990871404801486, 4.309630167425899, 4.990871404801486, 4.61066016308988, 4.7246035153967165, 5.025633511060698, 5.212720154417843, 6.973142637841055, 8.183996003155949, 3.9116901587538613, 6.104814757108323, 9.155967279555703, 4.849542252005016])\n",
      "dict_values([0.5, 0.625, 0.0, 0.125, 0.0, 0.0, 0.625, 0.125, 0.125, 0.125, 0.875, 1.0, 0.0, 0.25, 0.875, 0.125])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
      "dict_values([1, 0.6, -0.65, 0.5, 0.35, 0.9, 0.15, 0.5, 0.1, 0.9, 0.35, 0.95, -0.25, 0.95, 0.9, 1.0])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.25, 0.0, 0.75, 0.75, 0.5, 0.25, 1.0, 0.75, 0.0, 0.0, 0.5, 0.5, 1.0, 0.25, 0.5, 0.25])\n",
      "dict_values([1.0, 0.286, 0.429, 0.714, 0.714, 0.571, 0.857, 0.143, 0.286, 0.143, 0.143, 0.286, 0.857, 0.286, 0.429, 0.429])\n",
      "dict_values([3.853698211776175, 6.202392402041716, 9.020806832442615, 6.0842930899637215, 7.355359862250259, 4.800991861260171, 7.510261822236004, 5.005111843916096, 5.306141839580078, 3.0877814178095426, 6.862444340347365, 3.455758203104137, 8.860509840570165, 3.2127201544178425, 4.278113115979834, 5.790211954255068])\n",
      "dict_values([0.2, 0.3, 0.4, 0.9, 0.7, 0.1, 0.8, 0.6, 0.0, 0.0, 0.6, 0.3, 1.0, 0.0, 0.1, 0.2])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5])\n",
      "dict_values([1, 0.15, 1, 0.7, -0.5, 0.55, 1, 0.75, -1.4, 1, 0.85, 1, 1.0, 0.5, -0.45, 1, 0.55])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.111, 0.1, 0.111, 0.125, 0.143, 0.167, 0.5, 1.0, 0.333])\n",
      "dict_values([0.333, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 1.0, 0.0, 0.0])\n",
      "dict_values([1.0, 0.375, 0.125, 0.125, 0.5, 0.5, 0.125, 0.125, 0.625, 0.125, 0.375, 0.25, 0.25, 0.375, 0.625, 0.125, 0.375])\n",
      "dict_values([5.4416637207987995, 8.161823024204756, 2.681241237375587, 7.309885559660195, 9.894216784027726, 7.9699374979658435, 3.4771212547196626, 5.862727528317976, 12.587692477820326, 2.681241237375587, 8.337914283260437, 5.100370545117563, 4.839603729470837, 8.67430253418857, 10.353609271786956, 4.35564305022087, 8.67430253418857])\n",
      "dict_values([0.214, 0.214, 0.143, 0.143, 0.286, 0.357, 0.143, 0.0, 0.857, 0.0, 0.214, 0.0, 0.143, 0.214, 1.0, 0.0, 0.214])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "path1='C:\\\\Users\\\\jasme\\\\Desktop\\\\COVID_19_dataset\\\\documents\\\\'\n",
    "path2='C:\\\\Users\\\\jasme\\\\Desktop\\\\COVID_19_dataset\\\\summary\\\\'\n",
    "\n",
    "filelist = os.listdir(path1)\n",
    "for file in filelist:\n",
    "    f = open(path1+file, \"r\")\n",
    "    f1=open(path2+file,\"r\")\n",
    "    text=f.read()\n",
    "    text1=f1.read()\n",
    "    sent_tokens = nltk.sent_tokenize(text)\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    sent_tokens1 = nltk.sent_tokenize(text1)\n",
    "    word_tokens1 = nltk.word_tokenize(text1)\n",
    "    word_tokens_lower=[word.lower() for word in word_tokens]\n",
    "    stopWords = list(set(stopwords.words(\"english\")))\n",
    "    word_tokens_refined=[x for x in word_tokens_lower if x not in stopWords]\n",
    "    g=cue_phrases()\n",
    "    z=list(g.keys())\n",
    "    g=list(g.values())\n",
    "    h=numeric_data()\n",
    "    h=list(h.values())\n",
    "    i=sent_len_score()\n",
    "    i=list(i.values())\n",
    "    j=sentence_position()\n",
    "    j=list(j.values())   \n",
    "    p=upper_case()\n",
    "    p=list(p.values())\n",
    "    l=head_match()\n",
    "    l=list(l.values())\n",
    "    m=word_frequency()\n",
    "    m=list(m.values())\n",
    "    n=proper_noun()\n",
    "    n=list(n.values())\n",
    "    label={}\n",
    "    for sentence in sent_tokens:\n",
    "        label[sentence]=0\n",
    "        for sentence1 in sent_tokens1:\n",
    "            if(sentence==sentence1):\n",
    "                label[sentence]+=1\n",
    "                \n",
    "    o=list(label.values())\n",
    "    df = df.append(pd.DataFrame({'a': g,'b': h,'c': i,'d': j,'upper': p,'f': l,'g': m,'h': n,'key': z,'label': o}), ignore_index=True)\n",
    "    \n",
    "    f.close()\n",
    "    f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.to_csv('C:\\\\Users\\\\jasme\\\\Desktop\\\\COVID_19_dataset\\\\output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('C:\\\\Users\\\\jasme\\\\Desktop\\\\summarization_dataset\\\\output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=df[df.columns[0:7]]\n",
    "test=df[df.columns[8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(training, test, test_size=0.3,random_state=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jasme\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf2 = LogisticRegression(random_state=0)\n",
    "#Train the model using the training sets\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf2.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
